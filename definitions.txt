* External properties
lambda     : average arrival rate
1/lambda   : average time between arrivals

* Properties of a server (independent of traffic)
S     : size of a job - how long does it take to process in absence of other jobs
E[S]  : expected service time
μ     : average service rate

* Properties of a system (server + traffic)
T     : response time
E[T]  : mean response time
T_q   : waiting/queuing time
N_q   : number of jobs in the queue
N     : number of jobs in the system
ρ_i : Device utilization of device i (fraction of time busy)
X_i   : Throughput of device i (rate of completions)
Slowdown(j) : T(j)/S(j)

* Closed-system specific
Z     : think time (time between receiving end of 1 job and starting a new one)
R     : response time (time it takes system to do the work)
T     : system time

* Ergodocity of Markov Chains
m_ij: expected number of time steps needed to first get to j | we are at i
m_jj: expected number of time steps to get back to j | we are at j
π_j : limiting probability of ending up in state j
m_jj = 1/π_j
p_j : percentage of time spent in state j
f_j : probability that chain starting in state j ever returns to state j

period of state j: GCD of set of integers n, such that P_jj_n > 0
j is aperiodic iff period is 1
Chain is aperiodic iff all states are aperiodic
j is accessible from i if P_ij_n > 0 for some n > 0
i and j communicate iff i is accessible frm j and vice versa
Chain is irreducible iff all states communicate with one another
If f_j = 1, j is recurrent
If f_j < 1, j is transient
In irreducible Markov chains, all states are recurrent or transient
In positive-recurrent MC, mean m_ij is finite.
In null-recurrent MC, mean m_ij is infinite.




* Probability
P{E v F} = P{E} + P{F} - P{E ^ F}
P{E | F} = P{E ^ F}/P{F}
independence: P{E ^ F} = P{E} * P{F}
conditional independence: P{E ^ F | G} = P{E | G} * P{F | G}
Bayes: P{F | E} = P{E | F} * P{F} / P{E}
E[X]   : expected value : sum of (p * X)
Var(X) : E[(X - (E[X]))^2]
C^2_x  : squared coefficient of variation = Var(X)/(E[X]^2)
positive correlation: P{A|B} > P{A}
covariance(X,Y) = E[(X-E[X])(Y-E[Y])]
* Distributions:
Bernoulli(p): X = {1 with P=p; 0 with P=1-p} (coinflip)
Binomial(n, p): n independent Bernoullis, X = {i where i is number of Bernoulli 1's}
                (n choose i) * p^i * (1-p)^(n-i)
Geometric(x): number of Bernoulli trials until a 1 is reached
              (1-p)^(i-1)*p
Poisson(lambda): p(i) = e^-lambda * lambda^i / i!
Uniform(a, b): p(x) = 1/(a - b) if a<=x<=b
Exp(lambda): p(x) = lambda * e^(-lambda*x)
Pareto(a): p(x) = a * x^(-a-1)  (a usually 0<=a<2)

Equations:
* Open-system
E[T] = E[Tq] + E[S] (response time = queuing time + compute time)

* Closed-system
E[T] = E[R] + E[Z]

mu = 1/E[S]
X_i = mu_i * rho_i (service_rate * utilization = throughput)
rho_i = X_i * E[S] (Utilization Law) (utilization = throughput * compute time)
E[N] = lambda * E[T] (Little's Law)
N = X * E[T] (Little's Law for closed systems)
rho_i = lambda_i * E[S_i] = X_i * E[S_i]
E[R] = N/X - E[Z] (Response Time Law for Closed Systems)
X_i = E[V_i] * X (Forced flow)
rho_i = X * E[D_i]

Closed Systems:
X <= min(N/(D + E[Z]), 1/D_max)
E[R] >= max(D, (N * D_max) - E[Z])
N* = (D + E[Z]) / D_max

